{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP 527: Implementing the k-means clustering algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the assignment, you are required to cluster words belonging to four categories: animals, countries, fruits and veggies. The words are arranged into four different files. The first entry in each line is a word followed by 300 features (word embedding) describing the meaning of that word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    " \n",
    "> (1) Implement the k-means clustering algorithm with Euclidean distance to cluster the instances into k clusters. (30 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word():\n",
    "    \"\"\"Object class for a categorized word with data vector.\"\"\"\n",
    "    \n",
    "    def __init__(self, name, vector, category):\n",
    "        self.name = name\n",
    "        self.vector = vector\n",
    "        self.category = category\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'word: {self.name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(list_of_filenames):\n",
    "    \"\"\"Read in data.\"\"\"\n",
    "\n",
    "    collection = []\n",
    "\n",
    "    for filename in list_of_filenames:\n",
    "        data = open(filename).read().split('\\n')[:-1]\n",
    "\n",
    "        for word_data in data:\n",
    "            split = word_data.split(' ')\n",
    "            name = split[0]\n",
    "            raw_list = split[1:]\n",
    "\n",
    "            floats = []\n",
    "            for x_string in raw_list:\n",
    "                floats.append(float(x_string))\n",
    "\n",
    "            vector = np.array(floats)\n",
    "\n",
    "            collection.append( Word(name, vector, filename))\n",
    "\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329\n"
     ]
    }
   ],
   "source": [
    "categories = ['animals', 'countries', 'fruits', 'veggies']\n",
    "words = read_data(categories)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animals 50\n",
      "countries 161\n",
      "fruits 58\n",
      "veggies 60\n"
     ]
    }
   ],
   "source": [
    "category = {}\n",
    "for c in categories:\n",
    "    category[c] = []\n",
    "    for w in words:\n",
    "        if w.category == c:\n",
    "            category[c].append(w)\n",
    "    print(c, len(category[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    if w.vector.shape != words[0].vector.shape:\n",
    "        print('ERROR', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.015926, -0.079864])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0].vector[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(data):\n",
    "    \"\"\"Return two-dimensional vectors.\"\"\"\n",
    "\n",
    "    flat = []\n",
    "\n",
    "    for d in data:\n",
    "        two_dim = d.vector[0:2]\n",
    "        flat.append(Word(d.name, two_dim, d.category))\n",
    "\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = flatten(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.015926, -0.079864])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(u, v):\n",
    "    \"\"\"Return Euclidean distance between two np.array vectors.\"\"\"\n",
    "\n",
    "    return np.sqrt( (u - v).dot( u - v ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance(u, v):\n",
    "    \"\"\"Return Manhattan distance between two np.array vectors.\"\"\"\n",
    "\n",
    "    w = u - v\n",
    "    distance = 0\n",
    "    for x in w:\n",
    "        distance += abs(x)\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    \"\"\"Return Cosine similarity of two np.array vectors.\"\"\"\n",
    "    \n",
    "    return u.dot(v)/( np.sqrt(u.dot(u)) * np.sqrt(v.dot(v)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    \"\"\"Return normalized vectors (ie. parallel vector with unit magnitude).\"\"\"\n",
    "    \n",
    "    normalized_data = []\n",
    "    \n",
    "    for d in data:\n",
    "        normalized_vector = d.vector / np.sqrt( d.vector.dot(d.vector) )\n",
    "        normalized_data.append(Word(d.name, normalized_vector, d.category))\n",
    "        \n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = euclidean_distance, manhattan_distance, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4142135623730951\n",
      "2\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for metric in metrics:\n",
    "    print(metric(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "class KMeans():\n",
    "    \n",
    "    def __init__(\n",
    "                self, \n",
    "                k = 4, \n",
    "                data = words, \n",
    "                metric = euclidean_distance, \n",
    "                normalize = False, \n",
    "                max_iterations = 10**3, \n",
    "                seed = None,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initialize KMeans Model.\n",
    "        \n",
    "        Args:\n",
    "            k (int): number of clusters to divide data into.\n",
    "            data (list): list of dicts which must each include\n",
    "                the keys 'name' (string) and 'vector' (np.ndarray).\n",
    "            metric (function): to measure distance between points.\n",
    "            normalize (Boolean): whether or not to normalize vectors.\n",
    "            iterations (int): when to stop if no convergence.\n",
    "            seed (int): for reproducible (pseudo-)randomness.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.k = k\n",
    "        \n",
    "        if normalize:\n",
    "            self.data = normalize(data)\n",
    "        else:\n",
    "            self.data = data\n",
    "        \n",
    "        self.metric = metric\n",
    "        \n",
    "        self._upperbound, self._lowerbound = self._bounds()\n",
    "        \n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # we track centroid positions and cluster labels in nested dicts,\n",
    "        # of the form dict_name[centroid_number][iteration_number]\n",
    "        self._centroid = {}\n",
    "        for centroid_number in range(k):\n",
    "            self._centroid[centroid_number] = {}\n",
    "        \n",
    "        self._cluster = {}\n",
    "\n",
    "        # we record cluster labels explicitly as well\n",
    "        self._label = {}\n",
    "        for datum in data:\n",
    "            self._label[datum.name] = {}\n",
    "        \n",
    "        self.max_iterations = max_iterations\n",
    "        for i in range(self.max_iterations):\n",
    "            self._iteration = i\n",
    "            self._iterate()\n",
    "            if i > 0 and self._cluster[i] == self._cluster[i-1]:\n",
    "                break\n",
    "            \n",
    "        \n",
    "    def _bounds(self):\n",
    "        \"\"\"Find upper and lower bounds of data space.\"\"\"\n",
    "        \n",
    "        upper = np.zeros(len(words[0].vector))\n",
    "        lower = np.zeros(len(words[0].vector))\n",
    "        \n",
    "        for d in self.data:\n",
    "            for i, x in enumerate(d.vector):\n",
    "                upper[i] = max(upper[i], x)\n",
    "                lower[i] = min(lower[i], x)\n",
    "        \n",
    "        return upper, lower\n",
    "    \n",
    "    \n",
    "    def _start(self):\n",
    "        \"\"\"Generate starting positions for k centroids.\"\"\"\n",
    "        \n",
    "        for centroid_number in range(self.k):\n",
    "            self._centroid[centroid_number][0] = self._lowerbound \\\n",
    "                + np.random.random() * (self._upperbound - self._lowerbound)\n",
    "        print('centroids successfully positioned')\n",
    "    \n",
    "    \n",
    "    def _classify(self):\n",
    "        \"\"\"Assign each data point to cluster of nearest centroid.\"\"\"\n",
    "        \n",
    "        self._cluster[self._iteration] = {}\n",
    "        for centroid_number in range(self.k):\n",
    "            self._cluster[self._iteration][centroid_number] = []\n",
    "        \n",
    "        for d in self.data:\n",
    "            distances = [] \n",
    "            \n",
    "            for centroid_number in range(self.k):\n",
    "                \n",
    "                distances.append(self.metric(d.vector, self._centroid[centroid_number][self._iteration]))\n",
    "            \n",
    "            closest_centroid = np.argmin(distances)\n",
    "            \n",
    "            self._cluster[self._iteration][closest_centroid].append(d)\n",
    "            self._label[d.name][self._iteration] = closest_centroid\n",
    "        \n",
    "        print(f'classification successful for iteration {self._iteration}')\n",
    "\n",
    "            \n",
    "    def _reposition(self):\n",
    "        \"\"\"Move centroids to mean of each cluster.\"\"\"\n",
    "        \n",
    "        for centroid_number in range(self.k):\n",
    "            \n",
    "            clustered = self._cluster[self._iteration - 1][centroid_number]\n",
    "            \n",
    "            if len(clustered) > 0:\n",
    "                vector_sum = np.zeros(len(clustered[0].vector))\n",
    "                \n",
    "                for datum in clustered:\n",
    "                    vector_sum += datum.vector\n",
    "\n",
    "                cluster_mean = vector_sum / len(clustered)\n",
    "\n",
    "                self._centroid[centroid_number][self._iteration] = cluster_mean\n",
    "\n",
    "            else:\n",
    "                # nothing assigned to this cluster\n",
    "                self._centroid[centroid_number][self._iteration] = self._centroid[centroid_number][self._iteration -1] \n",
    "        \n",
    "        print(f'repositioning successful for iteration {self._iteration}')\n",
    "            \n",
    "            \n",
    "    def _stop():\n",
    "        \"\"\"Stop iterating and return results.\"\"\"\n",
    "        \n",
    "        return 'done'\n",
    "    \n",
    "    def _iterate(self):\n",
    "        \"\"\"Position centroids and classify data by nearest centroid.\"\"\"\n",
    "        \n",
    "        if self._iteration == 0:\n",
    "            self._start()\n",
    "        else:\n",
    "            self._reposition()\n",
    "        \n",
    "        if self._iteration == self.max_iterations:\n",
    "            self._stop()\n",
    "        else:\n",
    "            self._classify()\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centroids successfully positioned\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,) (300,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-399c3c6ccbab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-72d05968b307>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, k, data, metric, normalize, max_iterations, seed)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cluster\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cluster\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-72d05968b307>\u001b[0m in \u001b[0;36m_iterate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-72d05968b307>\u001b[0m in \u001b[0;36m_classify\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcentroid_number\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_centroid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcentroid_number\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iteration\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mclosest_centroid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-7089088d39f5>\u001b[0m in \u001b[0;36meuclidean_distance\u001b[0;34m(u, v)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Return Euclidean distance between two np.array vectors.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,) (300,) "
     ]
    }
   ],
   "source": [
    "model = KMeans(data=flatten(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper = np.zeros(len(words[0].vector))\n",
    "lower = np.zeros(len(words[0].vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = words\n",
    "for d in data:\n",
    "    for i, x in enumerate(d.vector):\n",
    "        upper[i] = max(upper[i], x)\n",
    "        lower[i] = min(lower[i], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'centroids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-bac3d91f80fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcentroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#= \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#                     lower + np.random.random() * (upper - lower)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'centroids' is not defined"
     ]
    }
   ],
   "source": [
    "centroids[0][0] #= \\\n",
    "#                     lower + np.random.random() * (upper - lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(-1,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[1].vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower[0] = min(lower[0], words[1].vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower[i] = min(lower[i], words[j].vector[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(5):\n",
    "    for i in range(5):\n",
    "        upper[i] = max([upper[i], words[j].vector[i]])\n",
    "        lower[i] = min([lower[i], words[j].vector[i]])\n",
    "    print(j, words[j].vector[0:5],'\\n', upper[0:5],'\\n', lower[0:5],'\\n', '\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w[1] = max(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(words[2].vector): print(i,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(words[0]['vector'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = '0.53533'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['animals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "]\n",
    "\n",
    "\n",
    "animal = open('animals').read().split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in animals:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = animals.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Computation\n",
    "\n",
    "> (2) Vary the value of k from 1 to 10 and compute the precision, recall, and F-score for each set of clusters. Plot k in the horizontal axis and precision, recall and F-score in the vertical axis in the same plot. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize\n",
    "\n",
    "> (3) Now re-run the k-means clustering algorithm you implemented in part (1) but normalise each feature vector to unit L2 length before computing Euclidean distances. Vary the value of k from 1 to 10 and compute the precision, recall, and F-score for each set of clusters. Plot k in the horizontal axis and precision, recall and F-score in the vertical axis in the same plot. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manhattan Distance\n",
    "\n",
    "> (4) Now re-run the k-means clustering algorithm you implemented in part (1) but this time use Manhattan distance over the unnormalised feature vectors. Vary the value of k from 1 to 10\n",
    "and compute the precision, recall, and F-score for each set of clusters. Plot k in the horizontal axis and precision, recall and F-score in the vertical axis in the same plot. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Manhattan Distance\n",
    "\n",
    "> (5) Now re-run the k-means clustering algorithm you implemented in part (1) but this time use Manhattan distance with L2 normalised feature vectors. Vary the value of k from 1 to 10 and\n",
    "compute the precision, recall, and F-score for each set of clusters. Plot k in the horizontal axis and precision, recall and F-score in the vertical axis in the same plot. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cosine Similarity\n",
    "\n",
    "> (6) Now re-run the k-means clustering algorithm you implemented in part (1) but this time use cosine similarity as the distance (similarity) measure.Vary the value of k from 1 to 10 andcompute the precision, recall, and F-score for each set of clusters. Plot k in the horizontal axis and precision, recall and F-score in the vertical axis in the same plot. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare and Discuss\n",
    "\n",
    "> (7) Comparing the different clusterings you obtained in (2)-(6) discuss what is the best setting for k-means clustering for this dataset. (20 marks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
