{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP 527: Implementing the k-means clustering algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the assignment, you are required to cluster words belonging to four categories: animals, countries, fruits and veggies. The words are arranged into four different files. The first entry in each line is a word followed by 300 features (word embedding) describing the meaning of that word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    " \n",
    "> (1) Implement the k-means clustering algorithm with Euclidean distance to cluster the instances into k clusters. (30 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word():\n",
    "    \"\"\"Object class for a categorized word with data vector.\"\"\"\n",
    "    \n",
    "    def __init__(self, name, vector, category):\n",
    "        self.name = name\n",
    "        self.vector = vector\n",
    "        self.category = category\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'word: {self.name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(list_of_filenames):\n",
    "    \"\"\"Read in data.\"\"\"\n",
    "\n",
    "    collection = []\n",
    "\n",
    "    for filename in list_of_filenames:\n",
    "        data = open(filename).read().split('\\n')[:-1]\n",
    "\n",
    "        for word_data in data:\n",
    "            split = word_data.split(' ')\n",
    "            name = split[0]\n",
    "            raw_list = split[1:]\n",
    "\n",
    "            floats = []\n",
    "            for x_string in raw_list:\n",
    "                floats.append(float(x_string))\n",
    "\n",
    "            vector = np.array(floats)\n",
    "\n",
    "            collection.append( Word(name, vector, filename))\n",
    "\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329\n"
     ]
    }
   ],
   "source": [
    "categories = ['animals', 'countries', 'fruits', 'veggies']\n",
    "words = read_data(categories)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animals 50\n",
      "countries 161\n",
      "fruits 58\n",
      "veggies 60\n"
     ]
    }
   ],
   "source": [
    "category = {}\n",
    "for c in categories:\n",
    "    category[c] = []\n",
    "    for w in words:\n",
    "        if w.category == c:\n",
    "            category[c].append(w)\n",
    "    print(c, len(category[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    if w.vector.shape != words[0].vector.shape:\n",
    "        print('ERROR', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.015926, -0.079864])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0].vector[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(data):\n",
    "    \"\"\"Return two-dimensional vectors.\"\"\"\n",
    "\n",
    "    flat = []\n",
    "\n",
    "    for d in data:\n",
    "        two_dim = d.vector[0:2]\n",
    "        flat.append(Word(d.name, two_dim, d.category))\n",
    "\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = flatten(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.015926, -0.079864])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(u, v):\n",
    "    \"\"\"Return Euclidean distance between two np.array vectors.\"\"\"\n",
    "\n",
    "    return np.sqrt( (u - v).dot( u - v ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance(u, v):\n",
    "    \"\"\"Return Manhattan distance between two np.array vectors.\"\"\"\n",
    "\n",
    "    w = u - v\n",
    "    distance = 0\n",
    "    for x in w:\n",
    "        distance += abs(x)\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    \"\"\"Return Cosine similarity of two np.array vectors.\"\"\"\n",
    "    \n",
    "    return u.dot(v)/( np.sqrt(u.dot(u)) * np.sqrt(v.dot(v)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    \"\"\"Return normalized vectors (ie. parallel vector with unit magnitude).\"\"\"\n",
    "    \n",
    "    normalized_data = []\n",
    "    \n",
    "    for d in data:\n",
    "        normalized_vector = d.vector / np.sqrt( d.vector.dot(d.vector) )\n",
    "        normalized_data.append(Word(d.name, normalized_vector, d.category))\n",
    "        \n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = euclidean_distance, manhattan_distance, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4142135623730951\n",
      "2\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for metric in metrics:\n",
    "    print(metric(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "class KMeans():\n",
    "    \n",
    "    def __init__(\n",
    "                self, \n",
    "                k = 4, \n",
    "                data = words, \n",
    "                metric = euclidean_distance, \n",
    "                normalize = False, \n",
    "                max_iterations = 10**3, \n",
    "                seed = None,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initialize KMeans Model.\n",
    "        \n",
    "        Args:\n",
    "            k (int): number of clusters to divide data into.\n",
    "            data (list): list of dicts which must each include\n",
    "                the keys 'name' (string) and 'vector' (np.ndarray).\n",
    "            metric (function): to measure distance between points.\n",
    "            normalize (Boolean): whether or not to normalize vectors.\n",
    "            iterations (int): when to stop if no convergence.\n",
    "            seed (int): for reproducible (pseudo-)randomness.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.k = k\n",
    "        \n",
    "        if normalize:\n",
    "            self.data = normalize(data)\n",
    "        else:\n",
    "            self.data = data\n",
    "        \n",
    "        self.metric = metric\n",
    "                \n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # we track centroid positions and cluster labels in nested dicts,\n",
    "        # of the form dict_name[iteration_number][centroid_number]\n",
    "        self._centroid = {}\n",
    "        \n",
    "        self._cluster = {}\n",
    "\n",
    "        # we record cluster labels explicitly as well\n",
    "        self._label = {}\n",
    "        \n",
    "        self.max_iterations = max_iterations\n",
    "        for i in range(self.max_iterations):\n",
    "            self._iteration = i\n",
    "            self._iterate()\n",
    "            if i > 0 and self._cluster[i] == self._cluster[i-1]:\n",
    "                break\n",
    "        \n",
    "        self.cluster = self._cluster[self._iteration]\n",
    "        self.label = self._label[self._iteration]\n",
    "        \n",
    "        \n",
    "        self._evaluate()\n",
    "   \n",
    "    def _start(self):\n",
    "        \"\"\"Shuffle dataset and position centroids on first k datapoints.\"\"\"\n",
    "        \n",
    "        selected = np.random.permutation(self.data)[0:self.k]\n",
    "        \n",
    "        self._centroid[0] = {}\n",
    "        \n",
    "        for centroid_number in range(self.k):\n",
    "            self._centroid[0][centroid_number] = selected[centroid_number].vector\n",
    "           \n",
    "    \n",
    "    def _classify(self):\n",
    "        \"\"\"Assign each data point to cluster of nearest centroid.\"\"\"\n",
    "        \n",
    "        self._cluster[self._iteration] = {}\n",
    "        self._label[self._iteration] = {}\n",
    "        \n",
    "        for centroid_number in range(self.k):\n",
    "            self._cluster[self._iteration][centroid_number] = []\n",
    "        \n",
    "        for d in self.data:\n",
    "            distances = [] \n",
    "            \n",
    "            for centroid_number in range(self.k):\n",
    "                \n",
    "                distances.append(self.metric(d.vector, self._centroid[self._iteration][centroid_number]))\n",
    "            \n",
    "            closest_centroid = np.argmin(distances)\n",
    "            \n",
    "            self._cluster[self._iteration][closest_centroid].append(d)\n",
    "            self._label[self._iteration][d.name] = closest_centroid\n",
    "        \n",
    "            \n",
    "    def _reposition(self):\n",
    "        \"\"\"Move centroids to mean of each cluster.\"\"\"\n",
    "        \n",
    "        for centroid_number in range(self.k):\n",
    "            self._centroid[self._iteration] = {}\n",
    "        \n",
    "        for centroid_number in range(self.k):\n",
    "            \n",
    "            clustered = self._cluster[self._iteration - 1][centroid_number]\n",
    "            \n",
    "            if len(clustered) > 0:\n",
    "                vector_sum = np.zeros(len(clustered[0].vector))\n",
    "                \n",
    "                for datum in clustered:\n",
    "                    vector_sum += datum.vector\n",
    "\n",
    "                cluster_mean = vector_sum / len(clustered)\n",
    "\n",
    "                self._centroid[self._iteration][centroid_number] = cluster_mean\n",
    "\n",
    "            else:\n",
    "                # nothing assigned to this cluster so position is unchanged\n",
    "                self._centroid[self._iteration][centroid_number] = \\\n",
    "                        self._centroid[self._iteration - 1][centroid_number]\n",
    "           \n",
    "   \n",
    "    def _iterate(self):\n",
    "        \"\"\"Position centroids and classify data by nearest centroid.\"\"\"\n",
    "        \n",
    "        if self._iteration == 0:\n",
    "            self._start()\n",
    "        else:\n",
    "            self._reposition()\n",
    "        \n",
    "        if self._iteration == self.max_iterations:\n",
    "            self._stop()\n",
    "        else:\n",
    "            self._classify()\n",
    "            \n",
    "            \n",
    "    def _evaluate(self):\n",
    "        \"\"\"Evaluate success of clustering.\"\"\"\n",
    "        \n",
    "        self.true_positives = 0\n",
    "        self.true_negatives = 0\n",
    "        self.false_positives = 0\n",
    "        self.false_negatives = 0\n",
    "        \n",
    "        D = self.data\n",
    "        \n",
    "        for i in range(len(D)):\n",
    "            for j in range(i+1, len(D)):\n",
    "                if D[i].category == D[j].category and self.label[D[i].name] == self.label[D[j].name]:\n",
    "                    self.true_positives += 1\n",
    "                if D[i].category != D[j].category and self.label[D[i].name] == self.label[D[j].name]:\n",
    "                    self.false_positives += 1\n",
    "                if D[i].category != D[j].category and self.label[D[i].name] != self.label[D[j].name]:\n",
    "                    self.true_negatives += 1\n",
    "                if D[i].category == D[j].category and self.label[D[i].name] != self.label[D[j].name]:\n",
    "                    self.false_positives += 1\n",
    "                    \n",
    "        \n",
    "        self.precision = self.true_positives / (self.true_positives + self.false_positives)\n",
    "        self.recall = self.true_positives / (self.true_positives + self.false_negatives)\n",
    "        self.f_score = 2 * self.precision * self.recall / (self.precision + self.recall)\n",
    "        \n",
    "        self.score = f' Precision: {self.precision}.\\n Recall: {self.recall}.\\n F-Score: {self.f_score}.'\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Precision, Recall, and F-Score\n",
    "\n",
    "> (2) Vary the value of k from 1 to 10 and compute the precision, recall, and F-score for each set of clusters. Plot k in the horizontal axis and precision, recall and F-score in the vertical axis in the same plot. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize\n",
    "\n",
    "> (3) Now re-run the k-means clustering algorithm you implemented in part (1) but normalise each feature vector to unit L2 length before computing Euclidean distances. Vary the value of k from 1 to 10 and compute the precision, recall, and F-score for each set of clusters. Plot k in the horizontal axis and precision, recall and F-score in the vertical axis in the same plot. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manhattan Distance\n",
    "\n",
    "> (4) Now re-run the k-means clustering algorithm you implemented in part (1) but this time use Manhattan distance over the unnormalised feature vectors. Vary the value of k from 1 to 10\n",
    "and compute the precision, recall, and F-score for each set of clusters. Plot k in the horizontal axis and precision, recall and F-score in the vertical axis in the same plot. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Manhattan Distance\n",
    "\n",
    "> (5) Now re-run the k-means clustering algorithm you implemented in part (1) but this time use Manhattan distance with L2 normalised feature vectors. Vary the value of k from 1 to 10 and\n",
    "compute the precision, recall, and F-score for each set of clusters. Plot k in the horizontal axis and precision, recall and F-score in the vertical axis in the same plot. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cosine Similarity\n",
    "\n",
    "> (6) Now re-run the k-means clustering algorithm you implemented in part (1) but this time use cosine similarity as the distance (similarity) measure.Vary the value of k from 1 to 10 andcompute the precision, recall, and F-score for each set of clusters. Plot k in the horizontal axis and precision, recall and F-score in the vertical axis in the same plot. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare and Discuss\n",
    "\n",
    "> (7) Comparing the different clusterings you obtained in (2)-(6) discuss what is the best setting for k-means clustering for this dataset. (20 marks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
